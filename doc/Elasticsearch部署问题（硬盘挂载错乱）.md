##部署说明

###硬件

```
服务器两台：
机器A：64G内存
机器B：32G内存
```

###分片

```
共12个节点
2个查询节点，10个存储节点
8个主分片
1个复制分片（每个分片都有一个副本分布在不同的节点上面）
每台机器都挂了6个机械盘每个盘都是不同的分区。
部署环境用docker weave 来做 elasticsearch cluster
就这样环境默默的部署着，一切都很顺利。
部署完成后开始导入数据，导入一部分数据后（由于数据量比较大），就关机下班了。大约有几十G左右。
第二点上班后要继续导入数据，开机，启动weave，启动Es 集群，
然后查看集群状态：red.  我想了下主分片不可用？不可能啊，是不是有的节点没有加入集群呢，然后查看weave 状态，两台机器都互相Ping一下 weave 转发的路由，一切都是通的，过了一会还是red,不淡定了，查看集群节点数量12个，对啊没错啊，怎么回事呢？然后又去看日志，没有报什么错误。现在真的开始不淡定了。
然后就用 _cat/shards,来查看到底是哪个分片出问题了。
结果看到7号分片没有挂载到集群里面
```

![](http://img.soaer.com/blog/1.jpg)
出现这个问题感觉很诡异啊，即使主分片找不到，副本也能顶上去啊。

这个问题发现越来越大了.

```
每个节点的数据都是挂载到不同的机械盘上的也就是不同的分区上面的。然后就一个一个分区的挨着找，然后查看分区数量也没错，进去分区中看看昨天传送数据的时候七号分片在那个盘上面。当找到第三个分区的时候发现 明明是两个节点的数据挂在这个分区上面怎么这个分区上面有五个节点的数据，哎开始头晕啊，没办法继续找原因吧，但是发现这个分区的两个节点中有七号分片，但是一点数据都没有，又看看其它的几个数据目录是正常的，然后紧跟着查看其它分区的情况发现第四个分区也是这样的情况，感觉跟发生了灵异事件差不多，命名配置的节点数据都在指定的分区上面（就是Docker挂在不通的目录下面），如果把第三个分区和第四分区的目录调换一下就应该是正确了。但是也不能手动的移动目录吧，还得找问题出在什么地方吧。
我用了下面的办法
在每个分区上面建立一个文件，文件名字就和分区编号一样，紧接着我关机，再次重新启动机器发现有的文件名称和自己所在的分区编号不一致（当然经过多次关机和开机测试的）。现在大家可能想到问题的根源了啊。
```

原来是挂盘出现的问题，最后把原来的盘符名称改成UUID后问题就解决了。
打开 /etc/fstab这个文件，把上面的/dev 开头的换成下面UUID开头的就行了

先在机器上面执行sudo blkid命令
然后按照对应关系进行替换，如下所示

```
stat and raid
/dev/sdb1	/opt/lucy1	ext4	defaults	0	0
/dev/sdc1	/opt/lucy2	ext4	defaults	0	0
/dev/sdd1	/opt/lucy3	ext4	defaults	0	0
/dev/sde1	/opt/lucy4	ext4	defaults	0	0
/dev/sdf1	/opt/raid	ext4	defaults	0	0
UUID=aaaaa-aaa-aaaa-aaa-aaaa	/opt/lucy1	ext4	defaults	0	0
UUID=bbbbb-bbb-bbbb-bbb-bbbb	/opt/lucy2	ext4	defaults	0	0
UUID=ccccc-ccc-cccc-ccc-cccc	/opt/lucy3	ext4	defaults	0	0
UUID=ddddd-ddd-dddd-ddd-dddd	/opt/lucy4	ext4	defaults	0	0
/dev/sdf1	/opt/raid	ext4	defaults	0	0
```