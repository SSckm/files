# 线上问题排查集合

## 端口占用情况

```
此命令可以看出处于所有状态的端口数量。
（主要排查端口是否占用完，没有可用的端口导致）
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'

TIME_WAIT 2689 （重点关注，如果数量很大，可以调整长链接连接池的大小，观察效果）
CLOSE_WAIT 2（重点关注，一般来说如果这个值过大或者说直线上升，则可以排查下程序看是否有请求BUG）
FIN_WAIT1 24
FIN_WAIT2 8
ESTABLISHED 468
```

## 硬盘排查

```
df -h 查看磁盘占用情况，一般情况主要看日志定期删除是否成功，会有大文件日志问题。
```

## CPU查看

```
top 指令，可以看到多核的CPU负载情况
```

## 内存查看

```
free -m 查看内存使用情况。
```

## Java线程占用CPU查看

```
如果不是系统问题（端口占用，网卡或者磁盘或者内部网络），我们需要查看Java线程运行情况
```

### 问题排查步骤（排查CPU过高问题）

```
1.top -H -p PID ， 找到占用CPU高的线程的tid
2.printf "%x\n" tid  将需要的线程ID转换为16进制格式
3.jstack pid |grep tid -A 30 用jstack 指令打印出该线程的状态以及方法调用情况

备注：一般如果是占用CPU过高有两种情况
1.应用确实是CPU计算密集型的应用。
2.程序出现死循环， 这种情况比较好定位问题，代码出错的行号也会在jstack命令里面显示。
3.在用jstack排查问题的时候一定要观察下是否出现死锁问题。
```

## Mysql的慢查询排查



## 内存方面排查

```
gc时间过长、OOM、死锁、线程block、线程数暴涨等问题
```

```
top命令：Linux命令。可以查看实时的内存使用情况。  
jmap -histo:live [pid]，然后分析具体的对象数目和占用内存大小，从而定位代码。
jmap -dump:live,format=b,file=xxx.xxx [pid]，然后利用MAT工具分析是否存在内存泄漏等等。

借助于eclipse 的 mat插件，可以根据具体的dump文件来查找大对象或者内存使用情况。
```

##使用jstat -gcutil命令查看进程的内存情况

```
jstat -gcutil 56438 2000 10

 S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT   
 35.30  32.62   9.38  42.47  59.85 598345 6521.021     6    0.350 6521.371
  0.00  32.62  58.42  42.47  59.85 598345 6521.021     6    0.350 6521.371
 33.41   0.00  34.53  42.47  59.85 598346 6521.035     6    0.350 6521.385
 33.41   0.00  98.11  42.47  59.85 598346 6521.035     6    0.350 6521.385
  0.00  22.31  72.77  42.47  59.85 598347 6521.046     6    0.350 6521.396
 38.65   0.00   5.43  42.47  59.85 598348 6521.055     6    0.350 6521.405
 38.65   0.00  65.34  42.47  59.85 598348 6521.055     6    0.350 6521.405
  0.00  32.09  45.35  42.47  59.85 598349 6521.065     6    0.350 6521.415
 30.56   0.00  14.25  42.47  59.85 598350 6521.076     6    0.350 6521.426
 30.56   0.00  86.13  42.47  59.85 598350 6521.076     6    0.350 6521.426
```

### OOM异常

```
发生OOM问题一般服务都会crash，业务日志会有OutOfMemoryError。OOM一般都是出现了内存泄露，需要查看OOM时候的jvm堆的快照，如果配置了-XX:+HeapDumpOnOutOfMemoryError, 在发生OOM的时候会在-XX:HeapDumpPath生成堆的dump文件，结合MAT，可以对dump文件进行分析，查找出发生OOM的原因
```

### 死锁

```
死锁原因是两个或者多个线程相互等待资源，现象一般是出现线程hung住，更严重会出现线程数暴涨，系统出现api alive报警等。查看死锁最好的方法就是分析当时的线程栈。
具体case 可以参考jstack命令里面的例子
用到的命令：
jps -v
jstack -l pid
```

### 线程block

```
jstack -l pid |wc -l
jstack -l pid |grep "BLOCKED"|wc -l
jstack -l pid |grep "Waiting on condition"|wc -l

线程block问题一般是等待io、等待网络、等待监视器锁等造成，可能会导致请求超时、造成线程数暴涨导致系统502等。

如果出现这种问题，主要是关注jstack 出来的BLOCKED、Waiting on condition、Waiting on monitor entry等状态信息。
如果大量线程在“waiting for monitor entry”：
可能是一个全局锁阻塞住了大量线程。

如果短时间内打印的 thread dump 文件反映，随着时间流逝，waiting for monitor entry 的线程越来越多，没有减少的趋势，可能意味着某些线程在临界区里呆的时间太长了，以至于越来越多新线程迟迟无法进入临界区。
如果大量线程在“waiting on condition”： 可能是它们又跑去获取第三方资源，迟迟获取不到Response，导致大量线程进入等待状态。 所以如果你发现有大量的线程都处在 Wait on condition，从线程堆栈看，正等待网络读写，这可能是一个网络瓶颈的征兆，因为网络阻塞导致线程无法执行。
```

###Awk

```
统计日志中IP出现的次数
awk '{i=$6;count[i]++}END{for(i in count)print(i,count[i])}' lexus-invoke.log
去重
cat lexus-invoke.log | awk '!a[$6]++{print}' | awk '{print $6}'
awk 时间范围的日志
cat lexus-invoke.log | awk '$2>"15:00:00,771" && $2<"16:00:00,771"'
第五列匹配112， 也就是说IP地址里面包含112的
awk '$6 ~ 112 {print NR, $6}' lexus-invoke.log
```

